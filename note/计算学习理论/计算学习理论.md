> 这篇博客内容不是很准确，只是提供概念的一些支持。


# 1 基础概念
&emsp;&emsp;计算学习理论 (computational learning theory) 研究的是关于通过"计算"来进行"学习"的理论,即关于机器学习的理论基础,其目的是分析学习任务的困难本质,为学习算法提供理论保证,并根据分析结果指导算法设计。
&emsp;&emsp;对于数据集$D={(x_1,y_1),...,(x_m,y_m)},y_i\in Y={-1,+1}$假设所有样本服从一个未知的分布$\mathcal{D}$，而且数据集能够保证独立同分布，则对于二分类问题泛化误差为（$h$为$\mathcal{X}$到$\mathcal{Y}$的一个映射）：
$$
E(h;\mathcal{D})=P_{x~\mathcal{D}}(h(x)\ne y)
$$
&emsp;&emsp;经验误差为：
$$
\hat{E}(h;D)=\frac{1}{m}\sum_{i=1}^mI(h(x_i\ne y_i))
$$
&emsp;&emsp;学习的目标是保证上面的误差小鱼误差参数$\epsilon$。
&emsp;&esmp;对于任意两个映射$h_1,h_2$可通过不合度量差别：
$$
d(h_1,h_2)=P_{x~\mathcal{D}}(h_1(x)\ne h_2(x))
$$

# 2 PAC学习
&emsp;&emsp;计算学习理论中最基本的是概率近似正确 (Probably Correct ,简称 PAC) 学习理论。
- PAC 辨识 (PAC Identify):对 0<ε , $\sigma < 1$,所有$c \in \mathcal{C}$和分布$\mathcal{D}$, 若存在学习算法,其输出假设$h\in \mathcal{H}$满足$P(E(h)\le \epsilon)\ge 1 - \sigma$ 则称该学习算法能从假设空间$\mathcal{H}$中PAC辨识概念类$\mathcal{C}$；
- PAC可学习:令$m$表示从分布$\mathcal{D}$中独立同分布采样的样例数目，$0<\epsilon, \sigma<1$，对所有分布$\mathcal{D}$，若存在算法和多项式$poly$使得对任何$m\ge poly(1/\epsilon,1/\sigma,size(x),size(c))$，该算法能从假设空间$\mathcal{H}$中PAC辨识类别$\mathcal{C}$，则称概念类$\mathcal{C}$对假设空间$\mathcal{H}$而言是PAC可学习的，也可称概念类$\mathcal{C}$是PAC可学习的；
- PAC 学习算法 (PAC Learning Algorithm): 若学习算法使概念类$\mathcal{C}$为 PAC可学习的,且该算法的运行时间也是多项式函数$poly(l/\epsilon , 1/\sigma ,size(x), size(c))$,则称概念类$\mathcal{C}$是高效PAC可学习 (efficiently PAC learnable)的,称算法£为概念类$\mathcal{C}$的PAC学习算法;
- 样本复杂度：满足PAC学习算法所需的$m\ge poly(1.\epsilon, 1/\sigma, size(x), size(c))$中最小的m称为学习算法的样本复杂度。

# 3 有限假设空间
## 3.1 可分情形
&emsp;&emsp;可分意味着目标概念$c$属于假设空间$\mathcal{H}$。学习策略：既然 D 中样例标记都是由目标概念c赋予的,并且c存在于假设空间$\mathcal{H}$中,那么,任何在训练集D上出现标记错误的假设肯定不是目标概念c.。于是,我们只需保留与D一致的假设,剔除与D不一致的假设即可。
&emsp;&emsp;若训练集 D 足够大,则可不断借助 D 中的样例剔除不一致的假设,直到究中仅下一个假设为止,这个假设就是目标概念 C。通常情形下,由于训练集规模有限,假设空间$\mathcal{H}$中可能存在不止一个与 D 一致的"等效"假设,对这些等效假设,无法根据 D 来对它们的优劣做进一步区分。对 PAC 学习来说,只要训练集D的规模能使学习算法以概率$1-\sigma$找到目标假设的 E 近似即可。
&emsp;&emsp;先估计泛化误差大于 ε 但在训练、集上仍表现完美的假设出现的概率。假定 h 的泛化误差大于 E ,对分布 D 上随机来样而得的任何样例 (x , y) , 有
$$
P(h(x)=y)=1-P(h(x)\ne y)=1-E(h)<1-\epsilon
$$
&emsp;&emsp;由于 D 包含 m 个从$\mathcal{D}$独立同分布采样而得的样例,因此, h 与 D 表现一致的概率为:
$$
P((h(x_1)=y_1)\bigwedge ... \bigwedge (h(x_m)=y_m))^m<(1-\epsilon)^m
$$
&emsp;&emsp;事先并不知道学习算法会输出哪个假设，仅需要保证泛华误差大于$\epsilon$，则在训练集上表现完美的所有假设的出现概率之和不大于$\sigma$即可:
$$
P(h\in \mathcal{H}:E(h))>\epsilon \bigwedge \hat{E}(h)=0<|\mathcal{H}|(1-\epsilon)^m<|\mathcal{H}e^{-m\epsilon}|
$$

$$
|\mathcal{H}|e^{-m\epsilon}\le \sigma
$$

$$
m\ge \frac{1}{\epsilon}(ln|\mathcal{H}|+ln\frac{1}{\sigma})
$$
&emsp;&emsp;此可知,有限假设空间$\mathcal{H}$都是 PAC 可学习的,所需的样倒数目即m,输出假设h 的泛化误差随样例数目的增多而收敛到0，收敛速率为$O(\frac{1}{m})$

## 3.2 不可分情形
&emsp;&emsp;对于较为困难的学习问题，目标概念$c$往往不存在于假设空间$\mathcal{H}$中，假定对于任何 $h \in \mathcal{H} \hat{E}(h)\ne0$，即$\mathcal{H}$中的任意一个假设都会在训练集上出现错误，根据Hoeffding不等式,$x_1,...,x_m$为m个独立随机变量且$0 \ge x_i\ge 1$,对任意$\epsilon > 0$有：
$$
P(\frac{1}{m}\sum_{i=1}^m x_i - \frac{1}{m}\sum_{i=1}^m \mathbb{E}(x_i)\ge\epsilon)\le e^{-2m\epsilon^2}
$$

$$
P(|\frac{1}{m}\sum_{i=1}^m x_i - \frac{1}{m}\sum_{i=1}^m \mathbb{E}(x_i)|\ge\epsilon)\le e^{-2m\epsilon^2}
$$
&emsp;&emsp;可知：
- 若训练集$D$包含$m$个从分布$\mathcal{D}$上独立同分布采样而得的样例，$0<\epsilon < 1$，则对任意$h\in \mathcal{H}$有：
$$
P(\hat{E}(h) - E(h) \ge \epsilon) \le e^{-2m\epsilon^2}
$$

$$
P(E(h) - \hat{E}(h) \ge \epsilon) \le e^{-2m\epsilon^2}
$$

$$
P(|E(h) - \hat{E}(h)| \ge \epsilon) \le 2e^{-2m\epsilon^2}
$$
- 若训练集$D$包含m个从分布$\mathcal{D}$上独立同分布采样而得的样例,$0<\epsilon<1$，则对任意$h\in \mathcal{H}$下式以至少$1-\sigma$的概率成立(样例数目 m 较大时 , h 的经验误差是其泛化误差很好的近似)：
$$
\hat{E}(h)-\sqrt{\frac{ln(2/\sigma)}{2m}}\le E(h)\le\hat{E}(h)+\frac{ln(2/\sigma)}{2m}
$$
- 若$\mathcal{H}$为有限假设空间，$0<\sigma<1$,对任意$h\in \mathcal{H}$:
$$
P(|E(h)-\hat{E}(h)|)\le \sqrt{\frac{ln|\mathcal{H}|+ln(2/\sigma)}{2m}}\ge 1 - \sigma
$$
- 不可支学习：令$m$表示分布$\mathcal{D}$中独立同分布采样得到的样例树木，$0<\epsilon,\sigma<1$，对于所有的分布$\mathcal{D}$若存在学习算法和多项式函数$poly$，使得$m\ge poly(1/\epsilon, 1/\sigma, size(x), size(c))$，该算法能从假设空间$\mathcal{H}$中输出满足下面公式的假设$h$:
$$
P(E(h)-min_{\hat{h}\in \mathcal{H}}E(\hat{h})\le \epsilon)\ge 1 - \sigma
$$
则称假设空间$\mathcal{H}$是不可知PAC可学习的。

# 4 VC维
&emsp;&emsp;现实学习任务所面临的通常是无限假设空间，VC维便是解决这个问题的方法。假设空间$\mathcal{H}$和示例集合$D={x_1,...,x_m}$,$\mathcal{H}$中每个假设h都能对$D$中示例赋予标记，标记结果科表示为$h|_{D}={(h(x_1),...,h(x_m))}$随着m的增大，$\mathcal{H}$中所有假设对D中示例所能赋予标记的可能结果数也会增大。
- 假设空间$\mathcal{H}$的增长函数$\prod_{\mathcal{H}}(m)$为：
$$
\prod_{\mathcal{H}}(m)=max_{{x_1,...,x_m}\subseteq \mathcal{X}}|{(h(x_1),...,h(x_m))|h\in \mathcal{H}}|
$$
&emsp;&emsp;增长函数表示假设空间对m个示例所能赋予标记的最大可能结果数，即$\mathcal{H}$对示例所能赋予标记的结果可能结果数越大，$\mathcal{H}$的表示能力越强，学习任务的适应能力越强。
- 对于假设空间$\mathcal{H}$,$0<\epsilon<1$和任意$h\in\mathcal{H}$有：
$$
P(|E(h)-\hat{E}(h)|>\epsilon)\le 4\prod_{\mathcal{H}}(2m)e^{-\frac{m\epsilon^2}{8}}
$$
- 假设空间$\mathcal{H}$的VC维是能被$\mathcal{H}$打散的最大示例集的大小，即：
$$
VC(\mathcal{H})=max{m:\prod_{\mathcal{H}}(m)=2^m}
$$
&emsp;&emsp;$VC(\mathcal{H})=d$表明存在大小为 d 的示例集能被假设空间对打散。
- 若假设空间$\mathcal{H}$的VC维为d，则：
$$
\prod_{\mathcal{H}}(m)\le \sum_{i=0}^d(m, i)(这里公式稍微有点出入)
$$
- 若假设空间$\mathcal{H}$的VC维为d，对任意整数$m\ge d$则：
$$
\prod_{\mathcal{H}}(m)\le (\frac{e\cdot m}{d})^d
$$
- 若假设空间$\mathcal{H}$的VC维为d，则对任意$m>d,0<\sigma<1$和$h\in \mathcal{H}$有：
$$
P(E(h)-\hat{E}(h)\le \sqrt{\frac{8dln{\frac{2em}{d}}+8ln{\frac{4}{\sigma}}}{m}})\ge 1 - \sigma
$$
- 任何 VC 维有限的假设空间$\mathcal{H}$都是(不可知 )PAC 可学习的。

# 5 Rademacher复杂度
&emsp;&emsp;Rademacher 复杂度 (Rademacher complexity) 是另一种刻画假设空间复杂度的途径,与 vc 维不同的是,它在一定程度上考虑了数据分布。
- 函数空间$\mathcal{F}$关于Z的经验Radermacher复杂度：
$$
\hat{R}_z(\mathcal{F})=\mathbb{E}_{\sigma}[sup_{f\in \mathcal{F}}\frac{1}{m}\sum_{i=1}^m\sigma_i f(z_i)]
$$
- 函数空间$\mathcal{F}$在分布$\mathcal{D}$上deRadermacher复杂度：
$$
R_m(\mathcal{F})=\mathbb{E}_{Z\subseteq Z;|Z|=m}=m[\hat{R}_Z(\mathcal{F})]
$$

# 6 稳定性
&emsp;&emsp;无论是基于 vc 维还是 Rademacher 复杂度来推导泛化误差界,所得到的结果均与具体学习算法无关,对所有学习算法都适用.这使得人们能够脱离具休学习算法的设计来考虑学习问题本身的性质,但在另一方面,若希望获得与算法有关的分析结果,则需另辟蹊径.稳定性 (stability) 分析是这方面一个值得关注的方向。