# 1 决策树
## 1.1 基本概念
&emsp;&emsp;决策树（Decision tree）由一个决策图和可能的结果（包括资源成本和风险）组成， 用来创建到达目标的规划。决策树建立并用来辅助决策，是一种特殊的树结构。决策树是一个利用像树一样的图形或决策模型的决策支持工具，包括随机事件结果，资源代价和实用性。决策树属于只能非参数学习算法、可以用于解决(多)分类问题，回归问题。 回归问题的结果，叶子结点的平均值是回归问题的解。
&emsp;&emsp;根节点：决策树具有数据结构里面的二叉树、树的全部属性。
&emsp;&emsp;非叶子节点 ：（决策点） 代表测试的条件，数据的属性的测试。
&emsp;&emsp;叶子节点 ：分类后获得分类标记。
&emsp;&emsp;分支： 测试的结果。

## 1.2 决策树的分类
- 分类树分析是当预计结果可能为离散类型（例如三个种类的花，输赢等）使用的概念；
- 回归树分析是当局域结果可能为实数（例如房价，患者住院时间等）使用的概念；
- CART分析是结合了上述二者的一个概念。CART是Classification And Regression Trees的缩写。


## 1.3 决策树的构造
&emsp;&emsp;决策树的构造过程：
- 1. 以当前数据集为根节点；
- 2. 找出可以将数据划分的最大的特征，新建子节点将数据划分；
- 3. 如果结果未满足预期重复2过程生成新的子节点。

## 1.4 决策树的划分选择
### 1.4.1 信息增益
&emsp;&emsp;信息熵是度量样本集合纯度的一种指标，可用于数据划分。假定样本集合$D$中第$k$类样本所占的比例为$p_k(k=1,...,m)$则$D$的信息熵定义为：
$$
Ent(D)=-\sum_{k=1}^{m} {p_k log_2{p_k} }
$$
&emsp;&emsp;当$Ent(D)$的值越小，则$D$中的数据纯度越高，也就是说处于同一类的概率越高。
&emsp;&emsp;假定离散属性$a$有$V$个可能的取值:$a^1,...,a^{V}$，若使用属性$a$的数据进行划分会产生$V$个分支，则第$v$个分支包含了$D$中属性为$a^v$的样本，该样本集为$D^v$，则可根据下面的公式计算出信息增益：
$$
Gain(D, a)=Ent(D) - \sum_{v=1}^V{\frac{|D^v|} {|D|} {Ent(D^v)} }
$$
&emsp;&emsp;信息增益越大，则使用属性$a$进行划分所带来的数据纯度提升越大。因此可以使用$a=\argmax_{a∈A}Gain(D,a)$选择划分的性质。

### 1.4.2 增益率
&emsp;&emsp;信息增益准则对可取值数目较多的属性有所偏好，为减少这种
偏好可能带来的不利影响，可以使用增益率进行划分：
$$
Gain_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}
$$
$$
IV(a)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|}log_2{\frac{|D^v|}{|D|}}
$$
&emsp;&emsp;属性$a$可能取值数目越多$IV(a)$的值通常越大。
&emsp;&emsp;增益率准则对可取值数目较少的属性有所偏好。

### 1.4.3 基尼系数
$$
Gini(D)=\sum_{k=1}^{|Y|}{\sum_{k^{'}≠k}{p_k p_{k^{'}}}}=1-\sum_{k=1}^{|Y|}{p_k^2}
$$
&emsp;&emsp;Gini(D)反映了数据集$D$随机抽取两个样本其类别标记不一致的概率，$Gini(D)$越小，数据集$D$的纯度越高。
&emsp;&emsp;属性$a$的基尼系数：
$$
Gini_index(D,a)=\sum_{v=1}^{V}\frac{|D^v|}{|D|}{Gini(D^v)}
$$
&emsp;&emsp;使用基尼系数进行划分时采用$a=\argmin_{a∈A}{Gini_index(D,a)}$

## 1.5 决策树剪枝
&emsp;&emsp;决策树容易产生过拟合，因此需要使用剪枝处理避免过拟合。决策树剪枝基本分为预剪枝和后剪枝。
&emsp;&emsp;预剪枝是指在决策树生成过程中，对每个结点在划
分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划
分并将当前结点标记为叶结点。
&emsp;&emsp;后剪枝则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点。
&emsp;&emsp;后剪枝的计算量代价比预剪枝方法大得多，特别是在大样本集中，不过对于小样本的情况，后剪枝方法还是优于预剪枝方法。

## 1.6 多变量决策树
&emsp;&emsp;若把每个属性视为坐标空间中的一个坐标轴，则 d 个属性描述的样本就对应了 d 维空间中的一个数据点，对样本分类则意味着在这个坐标空间中寻
找不同类样本之间的分类边界.决策树所形成的分类边界有一个明显的特点:
轴平行 (axis-parallel) ，即它的分类边界由若干个与坐标轴平行的分段组成。
这样的分类边界使得学习结果有较好的可解释性，因为每一段划分都直接对应了某个属性取值.但在学习任务的真实分类边界比较复杂时，必须使用很多段划分才能获得较好的近似。若能使用斜的划分边界，如图中红色线段所示，则决策树模型将大为简化"多变量决策树" (multivariate decision tree) 就是能实现这样的"斜划分"甚至更复杂划分的决策树。每个非叶结点是一个形如$\sum^d_{i=1}{w_ia_i=t}$的线性分类器，其中$w_i$是属性$a_i$的权重，$w_i,t$可在该结点所含的样本集和属性集上学得。如下三张图所示，第一张是决策树，第二张是多变量决策树，第三张是二者对比：
![](disctree.png)
![](multd.png)
![](com.png)

## 1.6 总结
### 1.6.1 优点
- 决策树易于理解和实现.人们在通过解释后都有能力去理解决策树所表达的意义。
- 对于决策树，数据的准备往往是简单或者是不必要的.其他的技术往往要求先把数据一般化，比如去掉多余的或者空白的属性。
- 能够同时处理数据型和常规型属性。其他的技术往往要求数据属性的单一。
- 是一个白盒模型如果给定一个观察的模型，那么根据所产生的决策树很容易推出相应的逻辑表达式。
- 易于通过静态测试来对模型进行评测。表示有可能测量该模型的可信度。
- 在相对短的时间内能够对大型数据源做出可行且效果良好的结果。
### 1.6.2 缺点
- 对于那些各类别样本数量不一致的数据，在决策树当中信息增益的结果偏向于那些具有更多数值的特征。

# 2 决策树算法与实现
## 2.1 ID3
### 2.1.1 算法原理
&emsp;&emsp;ID3使用信息熵进行节点划分，具体过程如下：
- 1. 使用所有没有使用的属性并计算与之相关的样本熵值；
- 2. 选取其中熵值最小的属性；
- 3. 生成包含该属性的节点；
- 4. 重复上过程直到达到预期。

### 2.1.2 实现
&emsp;&emsp;下面为决策树代码：
```python
class desicion_tree(object):
    def __init__(self, info_func):
        self.info_func = info_func
        
    def vote_classify(self, cls_list):
        cls_count = {}
        for i in range(k):
            vote_label = self.label[distance[i]]
            cls_count[vote_label] = cls_count.get(vote_label, 0) + 1
            
        sorted_cls = sorted(cls_count.items(), key=operator.itemgetter(1), reverse=True)
        return sorted_cls[0][0]
        
    def split_dataset(self, data_set, axis, value):
        '''
        @brief  划分数据集
        @param  data_set    数据
        @param  axis        划分依据的特征
        @param  value       需要返回的feaure
        '''
        ret = []
        for vec in data_set:
            if vec[axis] == value:
                ret_vec = vec[:axis]
                ret_vec.extend(vec[axis+1:])
                ret.append(ret_vec)
                
        return ret
    
    def choose_best_feature(self, dataset):
        if self.info_func is shannon.get_ent:
            return self.choose_best_shannon(dataset)
        else:
            best_feature = 0
            max_info = 0
            for i in range(len(dataset[0]) - 1):
                info = self.info_func(dataset, i)
                if info > max_info:
                    max_info = info
                    best_feature = i
                    
            return best_feature
            
    def choose_best_shannon(self, dataset):
        '''
        @brief  选择最好的feature进行划分
        '''
        basic_ent = shannon.get_ent(dataset)
        best_feature = -1
        best_gain = 0.0
        for i in range(len(dataset[0]) - 1):
            feature_list = [ele[i] for ele in dataset]
            feature_list = set(feature_list)
            cur_ent = 0.0
            for value in feature_list:
                sub_dataset = self.split_dataset(dataset, i, value)
                prob = len(sub_dataset)/float(len(dataset))
                cur_ent += prob * shannon.get_ent(sub_dataset)
            
            gain = basic_ent - cur_ent
            if gain > best_gain:
                best_gain = gain
                best_feature = i
                
        return best_feature
    
    def create_tree(self, dataset, labels):
        cls_list = [ele[-1] for ele in dataset]
        if cls_list.count(cls_list[0]) == len(cls_list):
            return cls_list[0]
            
        if len(dataset[0]) == 1:
            return self.vote_classify(cls_list)
            
        best_feature = self.choose_best_feature(dataset)
        best_label = labels[best_feature]
        cur_tree = {best_label:{}}
        
        del labels[best_feature]
        feature_vals = [ele[best_feature] for ele in dataset]
        feature_vals = set(feature_vals)
        for val in feature_vals:
            sub_labels = labels[:]
            cur_tree[best_label][val] = self.create_tree(self.split_dataset(dataset, best_feature, val), sub_labels)
            
        return cur_tree
    
    '''
    决策树分类
    '''
    def classify(self, dec_tree, feature_labels, test_vec):
        root = dec_tree.keys()[0]
        child = dec_tree[root]
        feature_id = feature_labels.index(root)
        key = test_vec[feature_id]
        value = child[key]
        if isinstance(value, dict):
            cls_label = self.classify(value, feature_labels, test_vec)
        else:
            cls_label = value
        
        return cls_label
    
    '''
    存储决策树
    '''
    def save(self, cur_tree, file):
        '''
        @brief  存储树结构
        '''
        with open(file, 'w') as f:
            pickle.dump(cur_tree, f)
            
    def load(self, cur_tree, file):
        with open(file, 'r') as f:
            return pickle.load(f)
```
&emsp;&emsp;下面为信息熵等的计算代码：
```python
def get_ent(dataset):
    '''
    @brief  计算信息熵
    '''
    label_count = {}
    for vec in dataset:
        label = vec[-1]
        if label not in label_count.keys():
            label_count[label] = 0
        
        label_count[label] += 1
        
    ent = 0.0
    for key in label_count:
        for key in label_count:
            prob = float(label_count[key]) / len(dataset)
            ent -= prob * math.log(prob, 2)
    
    return ent
    
    
def get_gain(dataset, axis):
    '''
    @brief  计算数据集的信息增益
    @param  axis    当前属性
    @param  dataset 每一行数据的最后一个分量为类标
    '''
    ent = get_ent(dataset)
    values = [line[axis] for line in dataset]
    values = set(values)
    gain = ent
    for value in values:
        sub_dataset = [line for line in dataset if line[axis] == value]
        sub_ent = get_ent(sub_dataset)
        gain -= len(sub_dataset)/len(dataset) * sub_ent
        
    return gain
    
    
def get_iv(dataset, axis):
    values = [line[axis] for line in dataset]
    values = set(values)
    iv = 0.0
    for value in values:
        sub_dataset = [line for line in dataset if line[axis] == value]
        iv -= len(sub_dataset)/len(dataset)
        
    return iv
    
    
def get_gain_ratio(dataset, axis):
    '''
    @brief  计算增益率
    '''
    gain = get_gain(dataset, axis)
    iv = get_iv(dataset, axis)
    return gain / iv
        

def get_gini(dataset):
    '''
    @brief  计算基尼值
    '''
    label_count = {}
    for vec in dataset:
        label = vec[-1]
        if label not in label_count.keys():
            label_count[label] = 0
        
        label_count[label] += 1
        
    gini = 1
    for key in label_count:
        prop = label_count[key]/len(dataset)
        gini -= prop * prop
        
    return gini
    

def get_gini_ratio(dataset, axis):
    '''
    @brief  计算基尼系数
    '''
    values = [line[axis] for line in dataset]
    values = set(values)
    gini = 0.0
    for value in values:
        sub_dataset = [line for line in dataset if line[axis] == value]
        prop = len(sub_dataset) / len(dataset)
        gini += prop * get_gini(sub_dataset)
        
    return gini
```
### 2.1.3 可视化结果
&emsp;&emsp;以下按次序分别为使用信息熵，信息增益，增益率，基尼系数构建的决策树:
![](ent.png)
![](gain.png)
![](gain_ratio.png)
![](gini_ratio.png)

## 2.2 C4.5
### 2.2.1 算法原理
&emsp;&emsp;C4.5和ID3唯一不同之处是使用信息增益率进行数据划分，其他和ID3相同
### 2.2.2 代码实现
&emsp;&emsp;代码实现参考上面的ID3中使用信息增益进行构建的代码。