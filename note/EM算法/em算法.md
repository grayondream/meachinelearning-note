# 1 简介
&emsp;&emsp;最大期望算法（Expectation-maximization algorithm，又译期望最大化算法）在统计中被用于寻找，依赖于不可观察的隐性变量的概率模型中，参数的最大似然估计。
&emsp;&emsp;在统计计算中，最大期望（EM）算法是在概率模型中寻找参数最大似然估计或者最大后验估计的算法，其中概率模型依赖于无法观测的隐变量。最大期望算法经常用在机器学习和计算机视觉的数据聚类（Data Clustering）领域。最大期望算法经过两个步骤交替进行计算，第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值；第二步是最大化（M），最大化在E步上求得的最大似然值来计算参数的值。M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。

# 2 算法原理
&emsp;&emsp;令$X$为已观测变量，$Z$为隐变量，$Θ$表示模型参数，对参数$Θ$做极大拟然估计则应该最大化对数拟然：
$$
LL(Θ|X,Z)=ln P(X,Z|Θ)
$$
&emsp;&emsp;由于$Z$是隐变量无法直接求解，可以通过对$Z$计算期望，来最大化已观测数据的对数边际拟然：
$$
LL(Θ|X,Z)=ln P(X,Z|Θ)=ln∑_Z P(X,Z|Θ)
$$
&emsp;&emsp;EM算法的基本思想是：如果参数$Θ$已知，则可根据训练数据推断出最优隐变量$Z$的值(E步)；若$Z$已知，可以直接对参数$Θ$进行极大拟然估计(M步)。
&emsp;&emsp;EM算法的基本步骤：
- 1. 初始化分布参数
- 2. 重复EM直到收敛：
  - E步：以当前参数$Θ^t$推断隐变量分布$P(Z|X,Θ^t)$，并计算对数拟然$LL(Θ|X,Z)$关于$Z$的期望：
$$
Q(Θ|Θ^t)=𝔼_{Z|X,Θ^t}LL(Θ|X,Z)
$$
  - M步：寻找最大化期望拟然：
$$
Θ^{t+1}=\argmax_{Θ}Q(Θ|Θ^t)
$$ 
# 3 参考
[如何通俗理解EM算法](https://blog.csdn.net/v_july_v/article/details/81708386)
[EM维基百科](https://www.wanweibaike.com/wiki-%E6%9C%80%E5%A4%A7%E6%9C%9F%E6%9C%9B%E7%AE%97%E6%B3%95)